---
title: |-
  How UKWA Works
pagenum: 1
prev_page:
  url: /index.html
next_page:
  url: /using-ukwa-services/index.html
suffix: .md
search: crawl our access uk capture web crawler work curators records also urls warc files logs service wact null sites example preserve provide jobs domain playback content specification set used warcs does once data trends job allows separate specific archive only frequent material index collect visit least year per using crawling live into readers researchers browse past analyse historical tools scope curated crawlers metadata processed generate users information need pages url known search datasets available manage processes curation interface including non print legal deposit results lists gov publications false implements contents itself process separately various same keep obtained offer collections archived

comment: "***PROGRAMMATICALLY GENERATED, DO NOT EDIT. SEE ORIGINAL FILES IN /content***"
---

    <main class="jupyter-page">
    <div id="page-info"><div id="page-title">How UKWA Works</div>
</div>
    <div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="How-does-it-work?">How does it work?<a class="anchor-link" href="#How-does-it-work?"> </a></h2><p>We enable curators and collaborators to define what we should collect, and how often. We attempt to visit every UK website at least once a year, and for the sites the curators have identified we can visit much more frequently. For example, we collect news sites at least once per day.</p>
<p>We capture the websites using web crawling software, and converting the live sites into static records we can preserve for the future.</p>
<p>We use these records to reconstruct an imperfect facsimile of the original live site, allowing our readers and researchers to browse the UK web as it was in the past. We can also analyse these historical records as data, to extract historical trends, or to build access tools.</p>
<h2 id="Yes,-but-how-does-it-work?">Yes, but how does it <em>work</em>?<a class="anchor-link" href="#Yes,-but-how-does-it-work?"> </a></h2><ul>
<li>Capture:<ul>
<li>Curators provide us with the URLs of interest, and instructions on the scope and limits of the crawl.</li>
<li>A web crawler takes the list of curated crawl jobs and executes them.</li>
<li>All know UK URLs are also passed to a large domain crawl job once per year.</li>
<li>The crawlers produce standardised WARC files and logs that capture what happened.</li>
</ul>
</li>
<li>Preserve:<ul>
<li>The WARC files, logs and metadata are kept safe.</li>
</ul>
</li>
<li>Access:<ul>
<li>The WARC files are processed to generate suitable indexes so users and tools can locate the information they need.</li>
<li>A playback service allows individual pages to be viewed, if the URL is known.</li>
<li>A search service allows trends to be explored, and pages to be discovered, based on their content.</li>
<li>Datasets summarising facts drawn from the content are made available too.</li>
</ul>
</li>
</ul>
<h2 id="Fine,-but-how-does-it-work?">Fine, but <em>how</em> does it <em>work</em>?<a class="anchor-link" href="#Fine,-but-how-does-it-work?"> </a></h2><p>We manage the Capture, Preserve and Access phases as separate, loosely-coupled processes with clear interfaces.</p>
<h3 id="Capture">Capture<a class="anchor-link" href="#Capture"> </a></h3><h4 id="Curation">Curation<a class="anchor-link" href="#Curation"> </a></h4><p>Our curators work with our <a href="https://github.com/ukwa/w3act">W3ACT</a> (World Wide Web Annotation and Curation Tool), which provide a user interface for configuring the crawl. This interface is very specific to the UK Web Archive, including our Non-Print Legal Deposit constraints and our additional licensing workflow. This results in a lists of crawl jobs to be executed, which look like this:</p>

<pre><code>[
    {
        "title": "gov.uk Publications",
        "seeds": [
            "https://www.gov.uk/government/publications"
        ],
        "schedules": [
            {
                "startDate": 1438246800000,
                "frequency": "MONTHLY",
                "endDate": null
            }
        ],
        "ignoreRobotsTxt": false,
        "depth": "DEEP",
        "scope": "root",
        "watched": false,
        "loginPageUrl": null,
        "secretId": null,
        "documentUrlScheme": null,
        "logoutUrl": null,
        "id": 1
    },
    ...</code></pre>
<p>Crucially, while W3ACT implements a lot of processes specific to the UK Web Archive, this only affects the contents of the crawl job specification. The specification format itself is more generic, although the configuration options to reflect expectations of the capabilities of the crawler itself.</p>
<h4 id="The-Frequent-Crawler">The Frequent Crawler<a class="anchor-link" href="#The-Frequent-Crawler"> </a></h4><p>The 'frequent crawler' implements the curated crawl jobs, and depends only on the crawl job specification shown above. A set of Python scripts are used to manage a set of crawl jobs configured to this specification. The crawling process results in a set of WARC files that capture the data and logs that capture some details about what we did and didn't crawl (e.g. blocked by robots.txt, or by a crawl quota).</p>
<h4 id="The-Domain-Crawler">The Domain Crawler<a class="anchor-link" href="#The-Domain-Crawler"> </a></h4><p>The annual domain crawl is operated separately, and launched separately using lists of known URLs from various sources (including W3ACT records marked 'Domain Crawl Only').  If outputs WARCs and logs in just the same way as the Frequent Crawler.</p>
<h3 id="Preserve">Preserve<a class="anchor-link" href="#Preserve"> </a></h3><p>We keep the WARCs and log files from all the crawlers, in a folder structure that reflects the crawl process. We keep content obtained under different terms in separate folders. For example, openly available Non-Print Legal Deposit material is separate from material that we obtained by permission, or from behind a paywall.</p>
<h3 id="Access">Access<a class="anchor-link" href="#Access"> </a></h3><p>Combining the WARCs and logs with metadata from curators, we offer various types of access.</p>
<h4 id="Playback">Playback<a class="anchor-link" href="#Playback"> </a></h4><p>The primary access mode is via a Playback service. This relies on readers knowing which URLs they wish to access, and a Content Index (or CDX) service is used to look up which WARC records hold copies of each URL, and from what times.</p>
<h4 id="Browse-&amp;-Search">Browse &amp; Search<a class="anchor-link" href="#Browse-&amp;-Search"> </a></h4><p>We take a range of steps to provide useful access to our collections for users who do not know which specific URLs they need. We use W3ACT to gather archived material into a set of browsable collections, and create a full-text index so the contents of the archive can be searched. This index can also be used to explore trends and perform some basic analyses.</p>
<h4 id="Datasets,-Statistical-Reports-&amp;-APIs">Datasets, Statistical Reports &amp; APIs<a class="anchor-link" href="#Datasets,-Statistical-Reports-&amp;-APIs"> </a></h4><p>We also work with researchers to provide data sets that summarise some factual aspects of the archived resources. For example, in the past we have processed our WARCs to analyse links between web sites, and how they change over time. The same infrastructure can be used to generate statistical reports.</p>
<p>Where appropriate, we also offer API access to information that it better queried than downloaded. For example our Playback service offers the Memento API, which allows others to query our holdings without downloading some massive dataset.</p>

</div>
</div>
</div>
</div>

 


    </main>
    